{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [00:07<00:00, 21242.17it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_data=pd.read_csv('train.csv',delimiter=',')\n",
    "train={}\n",
    "train_data=raw_data.values\n",
    "label_one=np.zeros(6)\n",
    "label_multi_count=Counter()\n",
    "for sample in tqdm(train_data):\n",
    "    id=sample[0]\n",
    "    context=sample[1]\n",
    "    labels=sample[2:]\n",
    "    label_multi_count.update([str(labels)])\n",
    "    for i,label in enumerate(labels):\n",
    "        label_one[i]+=label\n",
    "    train[id]={'context':context,'label':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[0 0 0 0 0 0]': 143346,\n",
       "         '[0 0 0 0 0 1]': 54,\n",
       "         '[0 0 0 0 1 0]': 301,\n",
       "         '[0 0 0 0 1 1]': 28,\n",
       "         '[0 0 0 1 0 0]': 22,\n",
       "         '[0 0 0 1 1 0]': 3,\n",
       "         '[0 0 1 0 0 0]': 317,\n",
       "         '[0 0 1 0 0 1]': 3,\n",
       "         '[0 0 1 0 1 0]': 181,\n",
       "         '[0 0 1 0 1 1]': 18,\n",
       "         '[0 0 1 1 0 0]': 2,\n",
       "         '[0 0 1 1 1 0]': 2,\n",
       "         '[1 0 0 0 0 0]': 5666,\n",
       "         '[1 0 0 0 0 1]': 136,\n",
       "         '[1 0 0 0 1 0]': 1215,\n",
       "         '[1 0 0 0 1 1]': 134,\n",
       "         '[1 0 0 1 0 0]': 113,\n",
       "         '[1 0 0 1 0 1]': 7,\n",
       "         '[1 0 0 1 1 0]': 16,\n",
       "         '[1 0 0 1 1 1]': 3,\n",
       "         '[1 0 1 0 0 0]': 1758,\n",
       "         '[1 0 1 0 0 1]': 35,\n",
       "         '[1 0 1 0 1 0]': 3800,\n",
       "         '[1 0 1 0 1 1]': 618,\n",
       "         '[1 0 1 1 0 0]': 11,\n",
       "         '[1 0 1 1 1 0]': 131,\n",
       "         '[1 0 1 1 1 1]': 56,\n",
       "         '[1 1 0 0 0 0]': 41,\n",
       "         '[1 1 0 0 0 1]': 3,\n",
       "         '[1 1 0 0 1 0]': 14,\n",
       "         '[1 1 0 0 1 1]': 7,\n",
       "         '[1 1 0 1 0 0]': 11,\n",
       "         '[1 1 0 1 0 1]': 1,\n",
       "         '[1 1 0 1 1 0]': 1,\n",
       "         '[1 1 1 0 0 0]': 158,\n",
       "         '[1 1 1 0 0 1]': 6,\n",
       "         '[1 1 1 0 1 0]': 989,\n",
       "         '[1 1 1 0 1 1]': 265,\n",
       "         '[1 1 1 1 0 0]': 4,\n",
       "         '[1 1 1 1 1 0]': 64,\n",
       "         '[1 1 1 1 1 1]': 31})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_num_ratio=[label_num/len(train_data) for label_num in label_one]\n",
    "sum(label_multi_count.values())\n",
    "label_num_ratio\n",
    "label_multi_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153164/153164 [00:01<00:00, 82011.70it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_data=pd.read_csv('test.csv',delimiter=',')\n",
    "test_data=raw_data.values\n",
    "test={}\n",
    "label_test_one=np.zeros(6)\n",
    "label_multi_test_count=Counter()\n",
    "for sample in tqdm(test_data):\n",
    "    id=sample[0]\n",
    "    context=sample[1]\n",
    "    labels=sample[2:]\n",
    "    label_multi_test_count.update([str(labels)])\n",
    "    for i,label in enumerate(labels):\n",
    "        label_one[i]+=label\n",
    "    test[id]={'context':context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[]': 153164})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_num_test_ratio=[label_num/len(test_data) for label_num in label_test_one]\n",
    "sum(label_multi_test_count.values())\n",
    "label_num_test_ratio\n",
    "label_multi_test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00001cee341fdb12',\n",
       "       \"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\"], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json','w') as f_train, open('test.json','w') as f_test:\n",
    "    json.dump(train,f_train)\n",
    "    json.dump(test,f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/xiongfei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiongfei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/xiongfei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "loading complete\n"
     ]
    }
   ],
   "source": [
    "#数据清洗\n",
    "import nltk\n",
    "import re\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk.corpus import stopwords\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print('loading complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki NOOBS wiki N',\n",
       " 'label': array([1, 0, 0, 0, 0, 0], dtype=object)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=train['fb3a15d68a7e7ff2']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本预处理思路\n",
    "\"\"\"\n",
    "1,转换大小写\n",
    "2，分句\n",
    "3，分词，词型还原\n",
    "4，去停用词\n",
    "\"\"\"\n",
    "import string \n",
    "from nltk.corpus import wordnet as wn  \n",
    "from nltk.corpus import stopwords\n",
    "def SenToken(document):#分割成句子  \n",
    "    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')  \n",
    "    sents =sent_tokenizer.tokenize(document)  \n",
    "    return sents\n",
    "\n",
    "def CleanLines(line):\n",
    "    identify= line.maketrans(\n",
    "       # If you find any of these\n",
    "       \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!\\\"#$%&()*+,-./:;<=>?@[]^_`{|}~'\\\\\",\n",
    "       # Replace them by these\n",
    "       \"abcdefghijklmnopqrstuvwxyz                                          \")\n",
    "    #delEStr = string.punctuation+string.digits  #ASCII 标点符号，数字\n",
    "    cleanLine = line.translate(identify) #去掉ASCII 标点符号和空格\n",
    "    return cleanLine  \n",
    "\n",
    "def WordTokener(sent):#将单句字符串分割成词   \n",
    "    wordsInStr= nltk.word_tokenize(sent)  \n",
    "    return wordsInStr  \n",
    "\n",
    "def CleanWords(word_list):#去掉标点符号，长度小于3的词以及non-alpha词，小写化  \n",
    "    cleanWords=[] \n",
    "    cleanWords = [w for w in word_list if w not in stopwords.words('english') and 3<=len(w)]\n",
    "    return cleanWords \n",
    "\n",
    "def StemWords(cleanWordsList):  \n",
    "    stemWords=[]  \n",
    "#   porter = nltk.PorterStemmer()#有博士说这个词干化工具效果不好，不是很专业  \n",
    "#   result=[porter.stem(t) for t incleanTokens]   \n",
    "    stemWords=[wn.morphy(w) for w in cleanWordsList]  \n",
    "    return stemWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(text):\n",
    "    sents=SenToken(text)\n",
    "    sents_list=[]\n",
    "    for sent in sents:\n",
    "        clear_sent=CleanLines(sent)\n",
    "        word_list=WordTokener(clear_sent)\n",
    "        clear_word_list=CleanWords(word_list)\n",
    "        #stem_word_list=StemWords(clear_word_list)\n",
    "        word_str=' '.join(clear_word_list)\n",
    "        sents_list.append(word_str)\n",
    "    sents_str='#'.join(sents_list)\n",
    "    return sents_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keep chin#darwinism accepted night#appears sense aat origins sort cut hypothesis ancestors went swinging trees prairies eons later take sedentary life style could edit wikipedia face book etc day#one watch little kids beach#first time wave throws bottom elbow may well cry time wait get water#home sea edge environment hard wired genes#unfortunately rise sea level since times means evidence coastal communities may never found#keep plugging away old hypothesis fade away'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_text=preprocess_document(text)\n",
    "pre_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 36048/159571 [11:57<25:53, 79.50it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c6be28aacd62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpre_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpre_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'pre_context'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpre_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-dd4cbc1e8bbc>\u001b[0m in \u001b[0;36mpreprocess_document\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mclear_sent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCleanLines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mword_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWordTokener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mclear_word_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCleanWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m#stem_word_list=StemWords(clear_word_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mword_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear_word_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c8cb7eddb981>\u001b[0m in \u001b[0;36mCleanWords\u001b[0;34m(word_list)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCleanWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#去掉标点符号，长度小于3的词以及non-alpha词，小写化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcleanWords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mcleanWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcleanWords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c8cb7eddb981>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCleanWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#去掉标点符号，长度小于3的词以及non-alpha词，小写化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcleanWords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mcleanWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcleanWords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \"\"\"\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such file or directory: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/posixpath.py\u001b[0m in \u001b[0;36mabspath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/posixpath.py\u001b[0m in \u001b[0;36mnormpath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0minitial_slashes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0mcomps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0mnew_comps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      " 23%|██▎       | 36048/159571 [12:10<41:41, 49.37it/s]"
     ]
    }
   ],
   "source": [
    "pre_train={}\n",
    "i=0\n",
    "for id,sample in tqdm(train.items()):\n",
    "    i+=1\n",
    "    context=sample['context']\n",
    "    pre_context=preprocess_document(context)\n",
    "    pre_train[id]={'pre_context':pre_context,'label':sample['label']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': array([0, 0, 0, 0, 0, 0], dtype=object), 'pre_context': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
